Creating servers through terraform
===================================
root@ip-172-31-81-209:~/Team3# cat mainec2.tf
variable "elb-names" {
  type = list
  default = ["Team3-kuber-master", "Team3-kuber-n1","Team3-Kuber-n2"]
}

variable "list" {
  type = list
  default = ["t2.medium","t2.medium","t2.medium"]
}


provider "aws" {
  region     = "us-east-1"
#  version = "5.60.0"
# access_key = "PUT-YOUR-ACCESS-KEY-HERE"
# secret_key = "PUT-YOUR-SECRET-KEY-HERE"
}

resource "aws_instance" "Team3-proj" {
ami = "ami-0e86e20dae9224db8"
count= 3

   instance_type = var.list[count.index]
tags= {
Name= var.elb-names[count.index]
}
# Block device mapping for the EBS volume
  root_block_device {
    volume_size = 20  # Size in GB
    volume_type = "gp2"  # General Purpose SSD
  }
}
root@ip-172-31-81-209:~/Team3# 
====================
root@ip-172-31-88-28:~/team3/ansible# cat basesetup.yaml 
---
- name:  Install Docker and Kubernetes on Master and workernode
  hosts: all  # This playbook runs on both master and worker nodes
  become: true  # Ensures the tasks are run with root privileges
  tasks:
    # Update the apt package list to ensure the latest packages are fetched
    - name: Update apt package list
      apt:
        update_cache: yes
    # Install Docker to manage containers, which is essential for Kubernetes nodes
    - name: Install Docker
      apt:
        name: docker.io
        state: present  # Ensures Docker is installed
    # Add the Kubernetes GPG key, which is required for securely downloading Kubernetes packages
    - name: Add Kubernetes GPG key
      shell: |
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      async: 60  # Maximum time in seconds to wait for this task to complete
      poll: 0    # Do not wait for completion; move to the next task
      ignore_errors: yes  # Continue if this task fails
    # Add the Kubernetes repository to the sources list to allow the installation of Kubernetes tools
    - name: Add Kubernetes apt repository
      shell: |
        echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
      async: 60
      poll: 0
      ignore_errors: yes
    # Update the package list again after adding the Kubernetes repository
    - name: Update apt cache after adding Kubernetes repo
      apt:
        update_cache: yes
    # Install kubelet, kubeadm, and kubectl
    - name: Install kubelet, kubeadm, and kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present  # Ensures all three tools are installed
      async: 60
      poll: 0
      ignore_errors: yes
    # Enable the bridge network calls, required for pod networking in Kubernetes
    - name: Enable net.bridge.bridge-nf-call-iptables
      sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: 1
        sysctl_set: yes  # Apply the setting
        state: present
      async: 60
      poll: 0
      ignore_errors: yes
root@ip-172-31-88-28:~/team3/ansible# 
==========================================
root@ip-172-31-88-28:~/team3/ansible# cat installcrid.yaml 
---
- name:  Install cri-dockerd
  hosts:  all
  become: true
  tasks:
    # Download and install cri-dockerd
    - name: Download cri-dockerd
      get_url:
        url: https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.14/cri-dockerd-0.3.14.amd64.tgz
        dest: /tmp/cri-dockerd-0.3.14.amd64.tgz
    - name: Extract cri-dockerd
      unarchive:
        src: /tmp/cri-dockerd-0.3.14.amd64.tgz
        dest: /tmp
        remote_src: yes
    - name: Install cri-dockerd binary
      command: install -o root -g root -m 0755 /tmp/cri-dockerd/cri-dockerd /usr/local/bin/cri-dockerd
    # Download and set up cri-dockerd systemd service
    - name: Download cri-dockerd systemd service files
      get_url:
        url: https://github.com/Mirantis/cri-dockerd/archive/refs/tags/v0.3.14.tar.gz
        dest: /tmp/cri-dockerd-v0.3.14.tar.gz
    - name: Extract cri-dockerd systemd files
      unarchive:
        src: /tmp/cri-dockerd-v0.3.14.tar.gz
        dest: /tmp
        remote_src: yes
    - name: Copy cri-dockerd systemd service files
      copy:
        src: /tmp/cri-dockerd-0.3.14/packaging/systemd/
        dest: /etc/systemd/system/
        remote_src: yes
    - name: Update cri-docker.service path
      replace:
        path: /etc/systemd/system/cri-docker.service
        regexp: '/usr/bin/cri-dockerd'
        replace: '/usr/local/bin/cri-dockerd'
    # Enable and start cri-docker service
    - name: Reload systemd daemon
      command: systemctl daemon-reload
    - name: Enable and start cri-docker.socket
      systemd:
        name: cri-docker.socket
        enabled: yes
        state: started
    - name: Enable and start cri-docker
      systemd:
        name: cri-docker
        enabled: yes
        state: started
    - name: Check cri-docker service status
      command: systemctl status cri-docker
      register: cri_docker_status
    - name: Display cri-docker service status
      debug:
        var: cri_docker_status.stdout_lines
=========================

 root@ip-172-31-88-28:~/team3/ansible# cat calicoinstall.yaml 
---
- name: Install calico on the kube master node
  hosts: master
  become: true
  tasks:
    # Apply the default Calico network plugin
    - name: Install Calico network plugin
      shell: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
    # Patch Calico to add the IP_AUTODETECTION_METHOD environment variable
    - name: Patch Calico node daemonset to set IP_AUTODETECTION_METHOD
      shell: |
        kubectl -n kube-system patch daemonset calico-node \
        --type=json \
        -p='[{"op": "add", "path": "/spec/template/spec/containers/0/env/-", "value": {"name": "IP_AUTODETECTION_METHOD", "value": "interface=enX0*"}}]'
      register: patch_output
      changed_when: "'patched' in patch_output.stdout"
root@ip-172-31-88-28:~/team3/ansible# 

==================
root@ip-172-31-88-28:~/team3/ansible# cat masterip.yaml.j2 
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.31.93.141  # Dynamically set the master node IP address
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
networking:
  podSubnet: 192.168.0.3/16
root@ip-172-31-88-28:~/team3/ansible#
============================
root@ip-172-31-88-28:~/team3/ansible# cat kubemasterinit.yaml 
---
- name: Initialize Kubernetes nodes
  hosts: master  # Run only on the master node
  become: true  # Run as root
  tasks:
    # Create the kube directory to store configuration files
    - name: Create kube directory
      file:
        path: /root/kube
        state: directory
        mode: '0755'
    # Create the Kubernetes configuration file needed for initializing the master node
    - name: Create Kubernetes configuration file
      template:
        src: /root/team3/ansible/masterip.yaml.j2
        dest: /root/kube/config.yaml
    # Initialize the Kubernetes master node using kubeadm and the configuration file
    - name: Initialize Kubernetes master with kubeadm
      shell: kubeadm init --config=/root/kube/config.yaml >> /root/kube/cluster_initialized
      register: master_init
      changed_when: "'Your Kubernetes control-plane has initialized successfully' in master_init.stdout"
    # Create the .kube directory to store Kubernetes configurations for the root user
    - name: Create .kube directory
      file:
        path: /root/.kube
        state: directory
        mode: '0755'
    # Copy the admin.conf file to the .kube directory for kubectl to interact with the cluster
    - name: Copy Kubernetes admin configuration
      command: cp /etc/kubernetes/admin.conf /root/.kube/config
    # Set proper ownership for the Kubernetes admin config file
    - name: Change ownership of Kubernetes admin config
      file:
        path: /root/.kube/config
        owner: root
        group: root
        mode: '0644'
    # Generate the kubeadm join command for worker nodes to join the cluster
    - name: Get join command for worker nodes
      shell: kubeadm token create --print-join-command
      register: join_command
    # Save the join command to a script for use on the worker nodes
    - name: Store join command for worker nodes
      copy:
        content: "{{ join_command.stdout }}"
        dest: /root/kube/join_command
        mode: '0755'
root@ip-172-31-88-28:~/team3/ansible#
===========================
root@ip-172-31-88-28:~/team3/ansible# cat workerjoin.yaml 
---
- name: Fetch and generate join configuration for worker nodes
  hosts: kubermaster
  become: yes
  tasks:
    # Fetch kubeadm join command from the master
    - name: Fetch the kubeadm join command
      command: kubeadm token create --print-join-command
      register: join_command_output
    # Debug the output to verify the join command
    - name: Display the kubeadm join command output
      debug:
        msg: "{{ join_command_output.stdout }}"
    # Extract token, API server endpoint, and CA cert hash using regex
    - name: Extract token, API server endpoint, and CA cert hash
      set_fact:
        bootstrap_token: "{{ join_command_output.stdout | regex_search('--token\\s+([\\w\\.]+)', '\\1') | first }}"
        api_server_endpoint: "{{ join_command_output.stdout | regex_search('kubeadm join\\s+([\\d\\.]+:[0-9]+)', '\\1') | first }}"
        ca_cert_hash: "sha256:{{ join_command_output.stdout | regex_search('--discovery-token-ca-cert-hash\\s+sha256:([a-fA-F0-9]+)', '\\1') | first }}"
    # Debug the extracted values to verify correctness
    - name: Debug the extracted bootstrap token, API server endpoint, and CA cert hash
      debug:
        msg: "Token: {{ bootstrap_token }}, API Server: {{ api_server_endpoint }}, CA Cert Hash: {{ ca_cert_hash }}"
    # Create the kubeadm JoinConfiguration file on the master node
    - name: Create kubeadm configuration file for worker nodes on master
      copy:
        content: |
          apiVersion: kubeadm.k8s.io/v1beta3
          kind: JoinConfiguration
          nodeRegistration:
            criSocket: unix:///var/run/cri-dockerd.sock  # Adjust based on your container runtime
          discovery:
            bootstrapToken:
              token: "{{ bootstrap_token }}"
              apiServerEndpoint: "{{ api_server_endpoint }}"
              caCertHashes:
                - "{{ ca_cert_hash }}"
        dest: /tmp/kubeadm-config.yaml
    # Copy the JoinConfiguration file from the master to the Ansible controller
    - name: Copy kubeadm configuration from master to Ansible controller
      fetch:
        src: /tmp/kubeadm-config.yaml
        dest: /tmp/kubeadm-config.yaml
        flat: yes
- name: Distribute join configuration to worker nodes and execute join command
  hosts: kuberworkers
  become: yes
  tasks:
    # Ensure /opt/kube directory exists on worker nodes
    - name: Ensure /opt/kube directory exists
      file:
        path: /opt/kube
        state: directory
        mode: '0755'
    # Install socat on worker nodes
    - name: Install socat on worker nodes
      package:
        name: socat
        state: present
    # Copy the JoinConfiguration file to the worker nodes
    - name: Copy JoinConfiguration file to worker nodes
      copy:
        src: /tmp/kubeadm-config.yaml
        dest: /opt/kube/kubeadm-config.yaml
    # Run the kubeadm join command on the worker nodes using the config file
    - name: Execute kubeadm join command on worker nodes
      shell: kubeadm join --config /opt/kube/kubeadm-config.yaml
      register: join_result
      ignore_errors: yes
    # Check the result of kubeadm join command
    - name: Check if kubeadm join was successful
      fail:
        msg: "Kubeadm join failed!"
      when: join_result.rc != 0
root@ip-172-31-88-28:~/team3/ansible#